*codecompanion.txt*       For NVIM v0.9.2       Last change: 2024 September 04

==============================================================================
Table of Contents                            *codecompanion-table-of-contents*

  - Features                                          |codecompanion-features|
  - Requirements                                  |codecompanion-requirements|
  - Installation                                  |codecompanion-installation|
  - Quickstart                                      |codecompanion-quickstart|
  - Configuration                                |codecompanion-configuration|
  - Advanced Usage                              |codecompanion-advanced-usage|
  - Extras                                              |codecompanion-extras|
  - Troubleshooting                            |codecompanion-troubleshooting|

FEATURES                                              *codecompanion-features*

- A Copilot Chat experience in Neovim
- Support for Anthropic, Copilot, Gemini, Ollama and OpenAI LLMs
- Inline transformations, code creation and refactoring
- Variables, Agents and Workflows to improve LLM output
- Built in prompts for LSP errors and code advice
- Ability to create your own custom prompts
- Async execution for improved performance


REQUIREMENTS                                      *codecompanion-requirements*

- The `curl` library installed
- Neovim 0.9.2 or greater
- _(Optional)_ An API key for your chosen LLM


INSTALLATION                                      *codecompanion-installation*

Install the plugin with your preferred package manager:

**Lazy.nvim**

>lua
    {
      "olimorris/codecompanion.nvim",
      dependencies = {
        "nvim-lua/plenary.nvim",
        "nvim-treesitter/nvim-treesitter",
        "hrsh7th/nvim-cmp", -- Optional: For activating slash commands and variables in the chat buffer
        "nvim-telescope/telescope.nvim", -- Optional: For working with files with slash commands
        {
          "stevearc/dressing.nvim", -- Optional: Improves the default Neovim UI
          opts = {},
        },
      },
      config = true
    }
<

**Packer**

>lua
    use({
      "olimorris/codecompanion.nvim",
      config = function()
        require("codecompanion").setup()
      end,
      requires = {
        "nvim-lua/plenary.nvim",
        "nvim-treesitter/nvim-treesitter",
        "hrsh7th/nvim-cmp", -- Optional: For activating slash commands and variables in the chat buffer
        "nvim-telescope/telescope.nvim", -- Optional: For working with files with slash commands
        "stevearc/dressing.nvim" -- Optional: Improves the default Neovim UI
      }
    })
<

**vim-plug**

>vim
    call plug#begin()
    
    Plug "nvim-lua/plenary.nvim"
    Plug "nvim-treesitter/nvim-treesitter"
    Plug "hrsh7th/nvim-cmp", " Optional: For activating slash commands and variables in the chat buffer
    Plug "nvim-telescope/telescope.nvim", " Optional: For working with files with slash commands
    Plug "stevearc/dressing.nvim" " Optional: Improves the default Neovim UI
    Plug "olimorris/codecompanion.nvim"
    
    call plug#end()
    
    lua << EOF
      require("codecompanion").setup()
    EOF
<


QUICKSTART                                          *codecompanion-quickstart*


  [!IMPORTANT] Okay, okay…it’s not quite a quickstart as you’ll need to
  configure your |codecompanion-adapter| first
**Inline Prompting**

The plugin has been designed so you only need to remember one command,
`:CodeCompanion`. Running `:CodeCompanion <your prompt>`, the plugin will
determine where to place your output (in a buffer or opening a chat buffer)
before responding to your prompt. You can even make a visual selection before
invoking the command to provide additional context to the LLM. You can find
more about |codecompanion-inline-prompting| in the section below.

Inline transformations
<https://github.com/olimorris/codecompanion.nvim/discussions/130> enable you to
use the context from a chat buffer in an inline prompt. A prompt such as
`:CodeCompanion add the new function here` can leverage the code that the LLM
has created in its last response in the chat buffer and write it at the cursor
position.

For convenience, you can also call |codecompanion-default-prompts| from the
command line:

- `/explain` - Explain how selected code in a buffer works
- `/tests` - Generate unit tests for selected code
- `/fix` - Fix the selected code
- `/buffer` - Send the current buffer to the LLM alongside a prompt
- `/lsp` - Explain the LSP diagnostics for the selected code
- `/commit` - Generate a commit message

Running `:'<,'>CodeCompanion /fix` will trigger the plugin to start following
the fix prompt as defined in the config
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/config.lua>.
Some of the slash commands can also take custom prompts. For example, running
`:'<,'>CodeCompanion /buffer refactor this code` sends the whole buffer as
context alongside a prompt to refactor the selected code. If you prompt the LLM
with something like `:CodeCompanion why are Lua and Neovim the perfect
combination?` then the prompt will be sent to the chat buffer instead.

Finally, there are keymaps available to accept or reject edits from the LLM in
the |codecompanion-inline-prompting| section.

**Chat Buffer**

The chat buffer is where you interact with LLMs from within Neovim. Running
`:CodeCompanionChat` or `:'<,'>CodeCompanionChat` will open one. As a
convenience, you can use `:CodeCompanionToggle` to toggle the visibility of a
chat buffer. Simply type prompt and press `<CR>` to send to the LLM.

When in the chat buffer you have access to a variety of data which can be sent
to the LLM as context. Variables, accessed via `#`, house data which relates to
the present state of Neovim:

- `#buffer` - Share the current buffer’s content with the LLM. You can also specify line numbers with `#buffer:8-20`
- `#editor` - Share the buffers and lines that you see in the editor’s viewport
- `#lsp` - Share LSP information and code for the current buffer

Slash commands, accessed via `/`, can be triggered to run commands to add data
into the chat buffer:

- `/buffer` - Select a loaded buffer to add its content to the chat buffer
- `/file` - Select a file from your working directory to add its content to the chat buffer

Both variables and slash commands can be extended in your config.


  [!TIP] When in the chat buffer, the `?` keymap brings up all of these available
  options.
**Agents / Tools**

In the chat buffer, you can turn LLMs into agents via the use of tools. In the
video above, we’re asking an LLM to execute the contents of the buffer via
the _@code_runner_ tool.

When in the chat buffer you have access to the following tools:

- `@code_runner` - The LLM can trigger the running of any code from within a Docker container
- `@rag` - The LLM can browse and search the internet for real-time information to supplement its response
- `@buffer_editor` - The LLM can edit code in a Neovim buffer by searching and replacing blocks


  [!IMPORTANT] Agents are still at an alpha stage.
**Action Palette**

The `:CodeCompanionActions` command will open the _Action Palette_, giving you
access to all of the functionality in the plugin. The _Prompts_ section is
where the default prompts and your custom ones can be accessed from.

You’ll notice that some prompts have a slash command in their description
such as `/commit`. This is a shortcut which allows you to trigger them from the
command line by doing `:CodeCompanion /commit`. Some of these prompts also have
keymaps assigned to them (which can be overwritten!) which offers an even
easier route to triggering them.


  [!NOTE] Some actions will only be visible in the _Action Palette_ if you’re
  in Visual mode.
**List of commands**

Below is a list of all of the commands that are available in the plugin:

- `CodeCompanion` - Inline prompt an LLM
- `CodeCompanion <your prompt>` - Can also inline prompt an LLM like this
- `CodeCompanion /<slash_cmd>` - Inline prompt an LLM with a slash command e.g. `/commit`
- `CodeCompanionChat` - Open up a chat buffer to converse with your LLM
- `CodeCompanionChat <adapter>` - Open up a chat buffer with a specific adapter
- `CodeCompanionToggle` - Toggle a chat buffer
- `CodeCompanionActions` - Open the _Action Palette_
- `CodeCompanionAdd` - Add visually selected chat to the current chat buffer

**Suggested workflow**

For an optimum workflow, I recommend the following options:

>lua
    vim.api.nvim_set_keymap("n", "<C-a>", "<cmd>CodeCompanionActions<cr>", { noremap = true, silent = true })
    vim.api.nvim_set_keymap("v", "<C-a>", "<cmd>CodeCompanionActions<cr>", { noremap = true, silent = true })
    vim.api.nvim_set_keymap("n", "<LocalLeader>a", "<cmd>CodeCompanionToggle<cr>", { noremap = true, silent = true })
    vim.api.nvim_set_keymap("v", "<LocalLeader>a", "<cmd>CodeCompanionToggle<cr>", { noremap = true, silent = true })
    vim.api.nvim_set_keymap("v", "ga", "<cmd>CodeCompanionAdd<cr>", { noremap = true, silent = true })
    
    -- Expand 'cc' into 'CodeCompanion' in the command line
    vim.cmd([[cab cc CodeCompanion]])
<


CONFIGURATION                                    *codecompanion-configuration*

Before configuring the plugin, it’s important to understand how it’s
structured.

The plugin uses adapters to connect to LLMs. Out of the box, the plugin
supports:

- Anthropic (`anthropic`) - Requires an API key and supports prompt caching <https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching>
- Copilot (`copilot`) - Requires a token which is created via `:Copilot setup` in Copilot.vim <https://github.com/github/copilot.vim>
- Gemini (`gemini`) - Requires an API key
- Ollama (`ollama`) - Both local and remotely hosted
- OpenAI (`openai`) - Requires an API key

The plugin also utilises objects called Strategies. These are the different
ways that a user can interact with the plugin. The _chat_ and _agent_
strategies harness a buffer to allow direct conversation with the LLM. The
_inline_ strategy allows for output from the LLM to be written directly into a
pre-existing Neovim buffer.

The plugin allows you to specify adapters for each strategy and also for each
|codecompanion-default-prompt|.


ADAPTERS ~

Please refer to your chosen adapter
<https://github.com/olimorris/codecompanion.nvim/tree/main/lua/codecompanion/adapters>
to understand its configuration. You will need to set an API key for
non-locally hosted LLMs.


  [!TIP] To create your own adapter or better understand how they work, please
  refer to the ADAPTERS <doc/ADAPTERS.md> guide.
**Changing the Default Adapter**

To specify a different adapter to the default (`openai`), simply change the
`strategies.*` table:

>lua
    require("codecompanion").setup({
      strategies = {
        chat = {
          adapter = "anthropic",
        },
        inline = {
          adapter = "copilot",
        },
        agent = {
          adapter = "anthropic",
        },
      },
    })
<

**Setting an API Key**

>lua
    require("codecompanion").setup({
      adapters = {
        anthropic = function()
          return require("codecompanion.adapters").extend("anthropic", {
            env = {
              api_key = "MY_OTHER_ANTHROPIC_KEY"
            },
          })
        end,
      },
    })
<

In the example above, we’re using the base of the Anthropic adapter but
changing the name of the default API key which it uses.

**Setting an API Key Using a Command**

Having API keys in plain text in your shell is not always safe. Thanks to this
PR <https://github.com/olimorris/codecompanion.nvim/pull/24>, you can run
commands from within your config. In the example below, we’re using the
1Password CLI to read an OpenAI credential.

>lua
    require("codecompanion").setup({
      adapters = {
        openai = function()
          return require("codecompanion.adapters").extend("openai", {
            env = {
              api_key = "cmd:op read op://personal/OpenAI/credential --no-newline",
            },
          })
        end,
      },
    })
<

**Using Ollama Remotely**

To use Ollama remotely, simply change the URL in the `env` table and set an API
key:

>lua
    require("codecompanion").setup({
      adapters = {
        ollama = function()
          return require("codecompanion.adapters").extend("ollama", {
            env = {
              url = "https://my_ollama_url",
              api_key = "OLLAMA_API_KEY",
            },
            headers = {
              ["Content-Type"] = "application/json",
              ["Authorization"] = "Bearer ${api_key}",
            },
            parameters = {
              sync = true,
            },
          })
        end,
      },
    })
<

**Connecting via a Proxy**

>lua
    require("codecompanion").setup({
      adapters = {
        opts = {
          allow_insecure = true, -- Use if required
          proxy = "socks5://127.0.0.1:9999"
        }
      },
    })
<

**Changing an Adapter’s Default Model**

>lua
    require("codecompanion").setup({
      adapters = {
        anthropic = function()
          return require("codecompanion.adapters").extend("anthropic", {
            schema = {
              model = {
                default = "claude-3-opus-20240229",
              },
            },
          })
        end,
      },
    })
<

**Configuring Adapter Settings**

LLMs have many settings such as _model_, _temperature_ and _max_tokens_. In an
adapter, these sit within a schema table and can be configured during setup:

>lua
    require("codecompanion").setup({
      adapters = {
        llama3 = function()
          return require("codecompanion.adapters").extend("ollama", {
            name = "llama3", -- Ensure this adapter is differentiated from Ollama
            schema = {
              model = {
                default = "llama3:latest",
              },
              num_ctx = {
                default = 16384,
              },
              num_predict = {
                default = -1,
              },
            },
          })
        end,
      },
    })
<


ADVANCED USAGE                                  *codecompanion-advanced-usage*


DEFAULT PROMPTS ~

The plugin comes with a number of default prompts, (as per the config
<https://github.com/olimorris/codecompanion.nvim/blob/main/lua/codecompanion/config.lua>),
which can be called via keymaps and/or slash commands. These prompts have been
carefully curated to mimic those in GitHub’s Copilot Chat
<https://docs.github.com/en/copilot/using-github-copilot/asking-github-copilot-questions-in-your-ide>.
Of course, you can create your own prompts and add them to the Action Palette
and as slash commands. Please see the RECIPES <doc/RECIPES.md> guide for more
information.


THE CHAT BUFFER ~

The chat buffer is where you converse with an LLM, directly from Neovim. It
behaves as a regular markdown buffer with some clever additions. When the send
keymap is pressed, the buffer’s content is sent to the LLM in the form of
prompts. When a response is received, it is then streamed directly into the
buffer. The chat buffer has been designed to be turn based, whereby the user
sends a message and the LLM replies. Messages are segmented by H2 headers. Once
a message has been sent, it cannot be edited.

As noted in the |codecompanion-quickstart| guide, you can leverage variables,
slash commands and tools to send additional context to the LLM.

**Keymaps**

When in the chat buffer, there are number of keymaps available to you:

- `?` - Bring up the options menu
- `<CR>`|`<C-s>` - Send the buffer to the LLM
- `<C-c>` - Close the buffer
- `q` - Cancel the request from the LLM
- `gr` - Regenerate the last response from the LLM
- `ga` - Change the adapter
- `gx` - Clear the buffer’s contents
- `gx` - Add a codeblock
- `gf` - To refresh the code folds in the buffer
- `}` - Move to the next chat
- `{` - Move to the previous chat
- `[` - Move to the next header
- `]` - Move to the previous header

**Settings**

If `display.chat.show_settings` is set to `true`, at the very top of the chat
buffer will be the adapter’s parameters which can be changed to tweak the
response from the LLM. You can find more detail by moving the cursor over them.

**Open Chats**

From the Action Palette, the `Open Chats` action enables users to easily
navigate between chat buffers. There are also keymaps available in the chat
buffer which enable easier navigation.


INLINE PROMPTING ~


  [!NOTE] If `send_code = false` in the config then this will take precedent and
  no code will be sent to the LLM
Inline prompts can be triggered via the `CodeCompanion <your prompt>` command.
As mentioned in the |codecompanion-getting-started| section, you can also
leverage chat buffer context, visual selections and slash commands like
`'<,'>CodeCompanion /buffer what does this code do?`, where the slash command
points to a |codecompanion-default-prompt| and any words after that act as a
custom prompt to the LLM.

One of the challenges with inline editing is determining how the LLM’s
response should be handled in the buffer. If you’ve prompted the LLM to
_“create a table of 5 common text editors”_ then you may wish for the
response to be placed at the cursor’s position in the current buffer.
However, if you asked the LLM to _“refactor this function”_ then you’d
expect the response to _replace_ a visual selection. The plugin will use the
inline LLM you’ve specified in your config to determine if the response
should…

- _replace_ - replace a visual selection you’ve made
- _add_ - be added in the current buffer at the cursor position
- _new_ - be placed in a new buffer
- _chat_ - be placed in a chat buffer

By default, inline prompting will trigger the diff feature, showing differences
between the original buffer and the changes from the LLM. This can be turned
off in your config. You an also choose to accept or reject the LLM’s
suggestions with:

- `ga` - Accept an inline edit
- `gr` - Reject an inline edit


AGENTS / TOOLS ~

As outlined by Andrew Ng in Agentic Design Patterns Part 3, Tool Use
<https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use>,
LLMs can act as agents by leveraging external tools. Andrew notes some common
examples such as web searching or code execution that have obvious benefits
when using LLMs.

In the plugin, agents are simply context that’s given to an LLM via a
`system` prompt. This gives it knowledge and a defined schema which it can
include in its response for the plugin to parse, execute and feedback on.
Agents can be added as a participant in a chat buffer by using the `@` key.

More information on how agents work and how you can create your own can be
found in the AGENTS <doc/AGENTS.md> guide.


WORKFLOWS ~


  [!WARNING] Workflows may result in the significant consumption of tokens if
  you’re using an external LLM.
As outlined <https://www.deeplearning.ai/the-batch/issue-242/> by Andrew Ng,
agentic workflows have the ability to dramatically improve the output of an
LLM. Infact, it’s possible for older models like GPT 3.5 to outperform newer
models (using traditional zero-shot inference). Andrew discussed
<https://www.youtube.com/watch?v=sal78ACtGTc&t=249s> how an agentic workflow
can be utilised via multiple prompts that invoke the LLM to self reflect.
Implementing Andrew’s advice, the plugin supports this notion via the use of
workflows. At various stages of a pre-defined workflow, the plugin will
automatically prompt the LLM without any input or triggering required from the
user.

Currently, the plugin comes with the following workflows:

- Adding a new feature
- Refactoring code

Of course you can add new workflows by following the RECIPES <doc/RECIPES.md>
guide.


EXTRAS                                                  *codecompanion-extras*

**Highlight Groups**

The plugin sets the following highlight groups during setup:

- `CodeCompanionChatHeader` - The headers in the chat buffer
- `CodeCompanionChatSeparator` - Separator between headings in the chat buffer
- `CodeCompanionChatTokens` - Virtual text in the chat buffer showing the token count
- `CodeCompanionChatTool` - Tools in the chat buffer
- `CodeCompanionChatVariable` - Variables in the chat buffer
- `CodeCompanionVirtualText` - All other virtual text in the plugin

**Events/Hooks**

The plugin fires many events during its lifecycle:

- `CodeCompanionChatClosed` - Fired after a chat has been closed
- `CodeCompanionChatAdapter` - Fired after the adapter has been set in the chat
- `CodeCompanionAgentStarted` - Fired when an agent has started using a tool
- `CodeCompanionAgentFinished` - Fired when an agent has finished using a tool
- `CodeCompanionInlineStarted` - Fired at the start of the Inline strategy
- `CodeCompanionInlineFinished` - Fired at the end of the Inline strategy
- `CodeCompanionRequestStarted` - Fired at the start of any API request
- `CodeCompanionRequestFinished` - Fired at the end of any API request


  [!TIP] Some events are sent with a data payload which can be leveraged. Please
  search the codebase for more information.
Events can be hooked into as follows:

>lua
    local group = vim.api.nvim_create_augroup("CodeCompanionHooks", {})
    
    vim.api.nvim_create_autocmd({ "User" }, {
      pattern = "CodeCompanionInline*",
      group = group,
      callback = function(request)
        if request.match == "CodeCompanionInlineFinished" then
          -- Format the buffer after the inline request has completed
          require("conform").format({ bufnr = request.buf })
        end
      end,
    })
<

**Statuslines**

You can incorporate a visual indication to show when the plugin is
communicating with an LLM in your Neovim configuration. Below are examples for
two popular statusline plugins.

_lualine.nvim:_

>lua
    local M = require("lualine.component"):extend()
    
    M.processing = false
    M.spinner_index = 1
    
    local spinner_symbols = {
      "⠋",
      "⠙",
      "⠹",
      "⠸",
      "⠼",
      "⠴",
      "⠦",
      "⠧",
      "⠇",
      "⠏",
    }
    local spinner_symbols_len = 10
    
    -- Initializer
    function M:init(options)
      M.super.init(self, options)
    
      local group = vim.api.nvim_create_augroup("CodeCompanionHooks", {})
    
      vim.api.nvim_create_autocmd({ "User" }, {
        pattern = "CodeCompanionRequest*",
        group = group,
        callback = function(request)
          if request.match == "CodeCompanionRequestStarted" then
            self.processing = true
          elseif request.match == "CodeCompanionRequestFinished" then
            self.processing = false
          end
        end,
      })
    end
    
    -- Function that runs every time statusline is updated
    function M:update_status()
      if self.processing then
        self.spinner_index = (self.spinner_index % spinner_symbols_len) + 1
        return spinner_symbols[self.spinner_index]
      else
        return nil
      end
    end
    
    return M
<

_heirline.nvim:_

>lua
    local CodeCompanion = {
      static = {
        processing = false,
      },
      update = {
        "User",
        pattern = "CodeCompanionRequest*",
        callback = function(self, args)
          if args.match == "CodeCompanionRequestStarted" then
            self.processing = true
          elseif args.match == "CodeCompanionRequestFinished" then
            self.processing = false
          end
          vim.cmd("redrawstatus")
        end,
      },
      {
        condition = function(self)
          return self.processing
        end,
        provider = " ",
        hl = { fg = "yellow" },
      },
    }
<

**Legendary.nvim**

The plugin also supports the amazing legendary.nvim
<https://github.com/mrjones2014/legendary.nvim> plugin. Simply enable it in
your config:

>lua
    require('legendary').setup({
      extensions = {
        codecompanion = true,
      },
    })
<


TROUBLESHOOTING                                *codecompanion-troubleshooting*

Before raising an issue
<https://github.com/olimorris/codecompanion.nvim/issues>, there are a number of
steps you can take to troubleshoot a problem:

**Checkhealth**

Run `:checkhealth codecompanion` and check all dependencies are installed
correctly. Also take note of the log file path.

**Turn on logging**

Update your config and turn debug logging on:

>lua
    opts = {
      log_level = "DEBUG", -- or "TRACE"
    }
<

and inspect the log file as per the location from the checkhealth command
(usually `~/.local/state/nvim/codecompanion.log`).

==============================================================================
1. Links                                                 *codecompanion-links*

1. *@code_runner_*: 

Generated by panvimdoc <https://github.com/kdheepak/panvimdoc>

vim:tw=78:ts=8:noet:ft=help:norl:
